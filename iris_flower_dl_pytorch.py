# -*- coding: utf-8 -*-
"""iris flower DL pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OhbLjtx-Ar0yG9xB2oS9Eyny48aUo94V
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd

# Define the model
class Model(nn.Module):
    # Input layer -> hidden layer 1 -> hidden layer 2 -> output layer
    def __init__(self, in_feature=4, h1=8, h2=9, out_feature=3):
        super(Model, self).__init__()
        self.fc1 = nn.Linear(in_feature, h1)  # First hidden layer
        self.fc2 = nn.Linear(h1, h2)          # Second hidden layer
        self.out = nn.Linear(h2, out_feature) # Output layer

    # Define the forward pass (this MUST be inside the class!)
    def forward(self, x):
        x = F.relu(self.fc1(x))  # First hidden layer with ReLU
        x = F.relu(self.fc2(x))  # Second hidden layer with ReLU
        x = self.out(x)          # Output layer (no activation, raw logits)
        return x

# Pick a manual seed for randomization
torch.manual_seed(41)

# Create an instance of the model
model = Model()

# Load the dataset
df = pd.read_csv('/content/IRIS.csv')

# Replace species names with numerical labels
df['species'] = df['species'].replace('Iris-setosa', 0)
df['species'] = df['species'].replace('Iris-versicolor', 1)
df['species'] = df['species'].replace('Iris-virginica', 2)

# Split the data into independent (x) and dependent (y) variables
x = df.drop(columns='species', axis=1).values
y = df['species'].values

# Train-test split
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=0)

# Convert features to tensors
x_train = torch.FloatTensor(x_train)
x_test = torch.FloatTensor(x_test)

# Convert labels to tensors
y_train = torch.LongTensor(y_train)
y_test = torch.LongTensor(y_test)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for classification
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Train the model
epochs = 100
losses = []

for i in range(epochs):
    # Forward pass: compute predictions
    prediction1 = model(x_train)

    # Compute the loss
    loss = criterion(prediction1, y_train)

    # Track the loss
    losses.append(loss.item())

    # Print the loss every 10 epochs
    if i % 10 == 0:
        print(f'Epoch Number: {i}, Loss: {loss.item()}')

    # Backpropagation and optimization
    optimizer.zero_grad()  # Zero gradients
    loss.backward()        # Backpropagation
    optimizer.step()       # Update weights

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
plt.plot(range(epochs),losses)
plt.xlabel('Epochs')
plt.ylabel('Loss')

#evaluate model on test dataset

with torch.no_grad():
  prediction2 = model(x_test)
  loss = criterion(prediction2,y_test)
  print(loss)

correct = 0
with torch.no_grad():
  for i , data in enumerate (x_test):
    y_pred = model(data)

if y_test[i] ==0 :
  print('Iris-setosa')
elif y_test[i] ==1 :
  print('Iris-versicolor')
elif y_test[i] ==2 :
  print('Iris-virginica')

#will tell what type of flower  , our neural network thinks
print(f'{i + 1} {str(y_pred)} { y_test[i]}{y_pred.argmax().item()}')

# correct or not
if y_pred.argmax().item==y_test[i]:
  correct +=1
print(f'we got {correct} correct!')

